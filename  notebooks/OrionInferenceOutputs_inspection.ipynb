{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPurGOtYyA9/pTT3SQ35RPZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **ORION - Outputs inspection**"],"metadata":{"id":"8OK6EWSDkbwv"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1uwP8AP8QxaA","executionInfo":{"status":"ok","timestamp":1763018529211,"user_tz":480,"elapsed":46271,"user":{"displayName":"Rahhul Jayaprakash","userId":"02142374991309415946"}},"outputId":"10a3ea66-8b34-46b5-f879-67839c9c9f3a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/RA-KaiKaiLiu/Orion')"],"metadata":{"id":"U1IIbhhnnCxD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Inference steps\n","\n","\n","\n","The complete inference flow is:\n","\n","* [test.py](https://github.com/xiaomi-mlab/Orion/blob/main/adzoo/orion/test.py) → loads config and checkpoint\n","* forward_test() → resets memory, calls simple_test_pts()\n","* extract_feat() → backbone + neck feature extraction\n","* Detection heads → pts_bbox_head.forward() and map_head.forward()\n","* VLM → lm_head.inference_ego() generates planning token\n","* Planning decoder → VAE/Diffusion/MLP generates trajectory\n","* Metrics → compute planning metrics (L2 error, collision, etc.)"],"metadata":{"id":"UPqwCRTsaL7Z"}},{"cell_type":"markdown","source":["During inference, the following methods in [orion.py](https://github.com/xiaomi-mlab/Orion/blob/2eddb627/mmcv/models/detectors/orion.py) are called in sequence:\n","<ul type=\"none\">\n","<li>a) forward_test() - Entry point for test-time inference</li>\n"," <ul>\n","<li>Resets memory if needed </li>\n","<li>Delegates to simple_test() </li>\n","</ul>\n","<li>b) simple_test() - Processes single scene</li>\n","<ul>\n","<li>Calls extract_img_feat() to extract image features</li>\n","<li>Calls simple_test_pts() for the main inference pipeline</li>\n","</ul>\n","<li>c) simple_test_pts() - Core inference pipeline</li>\n","<li>This is where the three-stage processing happens:</li>\n","<ul>\n","<li>Vision Stage: Calls pts_bbox_head() for object detection and map_head() for lane detection</li>\n","<li>Reasoning Stage: Calls lm_head.inference_ego() or lm_head.generate() for VLM processing</li>\n","<li>Action Stage: Depending on config, calls trajectory decoders:\n","<ul><li>VAE decoder: ego_fut_decoder()</li><li>Diffusion decoder: diff_decoder()</li><li>MLP decoder: waypoint_decoder()</li></ul></li>\n","</ul>"],"metadata":{"id":"-O_LSKsdbSy2"}},{"cell_type":"markdown","source":["# Vision Space\n","##Image Loading & Preprocessing\n","Pipeline: inference_only_pipeline in agent configuration [orion_stage3_agent.py](https://github.com/Rahhul17-IITH/Orion-v1/blob/main/adzoo/orion/configs/orion_stage3_agent.py)\n","\n","**Input:**\n","\n","* Raw images from CARLA: 6 camera views (RGB, variable resolution) [orion_b2d_agent.py](https://github.com/Rahhul17-IITH/Orion-v1/blob/main/team_code/orion_b2d_agent.py)\n","\n","**Processing:**\n","\n","* LoadMultiViewImageFromFilesInCeph: Load images to float32 [orion_stage3_agent.py](https://github.com/Rahhul17-IITH/Orion-v1/blob/main/adzoo/orion/configs/orion_stage3_agent.py)\n","* ResizeCropFlipRotImage: Apply image transforms (no augmentation during inference)\n","* ResizeMultiview3D: Resize to (640, 640)\n","* NormalizeMultiviewImage: Apply normalization\n","* PadMultiViewImage: Pad to divisible by 32\n","\n","**Output:**\n","\n","* results['img']: List of 6 images, stacked to (6, 3, 640, 640) tensor [orion_b2d_agent.py](https://github.com/Rahhul17-IITH/Orion-v1/blob/main/team_code/orion_b2d_agent.py)\n","\n","## Feature Extraction\n","**Function:** extract_img_feat() called from simple_test() [orion.py](https://github.com/xiaomi-mlab/Orion/blob/main/mmcv/models/detectors/orion.py)\n","\n","**Input:**\n","\n","* img: (B, N, C, H, W) = (1, 6, 3, 640, 640) tensor (dtype: float32 or float16)\n","\n","**Processing:**\n","\n","* Reshape: (B*N, C, H, W) = (6, 3, 640, 640)\n","* img_backbone.forward() (EVAViT): Extract features\n","* Reshape back: (B, N, C_feat, H_feat, W_feat)\n","\n","**Output:**\n","\n","* img_feats_reshaped: (1, 6, 1024, 40, 40) tensor - 1024-dim features at 40×40 resolution\n","\n","## Position Encoding\n","**Function:** position_embeding() in simple_test_pts() [orion.py](https://github.com/xiaomi-mlab/Orion/blob/main/mmcv/models/detectors/orion.py)\n","\n","**Input:**\n","\n","* img_feats: (1, 6, 1024, 40, 40)\n","* location: (6, 40, 40, 2) - 2D grid coordinates from prepare_location()\n","\n","**Processing:**\n","\n","* Flatten spatial: (B, N*H*W, C) = (1, 9600, 1024)\n","* Generate depth bins and encode via position_encoder MLP\n","\n","**Output:**\n","\n","* pos_embed: (1, 9600, 256) - positional embeddings\n","\n","## Object Detection\n","* Function: pts_bbox_head.forward() (OrionHead) [orion.py](https://github.com/xiaomi-mlab/Orion/blob/main/mmcv/models/detectors/orion.py)\n","\n","**Input:**\n","\n","* img_feats: (1, 6, 1024, 40, 40)\n","* pos_embed: (1, 9600, 256)\n","* img_metas: List with camera calibration, ego pose, etc.\n","\n","**Processing:**\n","\n","* Flatten & project: memory = input_projection(img_feats) → (1, 9600, 256)\n","* Initialize 600 object queries + 256 VLM tokens\n","* 6-layer transformer decoder with temporal memory\n","* Extract VLM tokens from last layer: (1, 256, 256)\n","* Project to 4096-dim: (1, 256, 4096)\n","* Add CAN bus embedding: (1, 257, 4096) (256 object + 1 planning token)\n","\n","**Output:**\n","\n","* outs_bbox: Dict with detection predictions (bboxes, classes, scores)\n","* det_query: (1, 257, 4096) tensor - vision tokens for VLM\n","\n","## Map Detection\n","* Function: map_head.forward() (OrionHeadM) [orion.py](https://github.com/xiaomi-mlab/Orion/blob/main/mmcv/models/detectors/orion.py)\n","\n","**Input:**\n","\n","* img_feats: (1, 6, 1024, 40, 40)\n","* pos_embed: (1, 9600, 256)\n","\n","**Processing:**\n","\n","* Similar to OrionHead but with 1800 lane queries + 256 VLM tokens\n","* 6-layer transformer decoder\n","* Extract & project VLM tokens: (1, 256, 4096)\n","\n","**Output:**\n","\n","* outs_lane: Dict with lane predictions (coordinates, classes)\n","* map_query: (1, 256, 4096) tensor - map vision tokens"],"metadata":{"id":"poDBG7bm3L4d"}},{"cell_type":"code","source":["#printing det_query, map_query\n","!python results-orion-outputs/scripts/visualize_vision_embeddings.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oAT9qOaVFdLV","executionInfo":{"status":"ok","timestamp":1763019938353,"user_tz":480,"elapsed":910,"user":{"displayName":"Rahhul Jayaprakash","userId":"02142374991309415946"}},"outputId":"6e32fec1-b5cc-44c9-d75b-4e67004348ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Object Embedding Shape: (1, 273, 4096)\n","Object Embedding Array:\n"," [[[ 0.15061843  1.785963   -0.5301314  ... -1.2414083   0.5657144\n","    0.26052558]\n","  [ 0.03301103  0.80292714 -0.33988827 ... -0.79655665  0.15823792\n","    0.54157895]\n","  [ 0.0091894   1.3270832  -0.52480584 ... -1.0784613   0.5990549\n","    0.6027265 ]\n","  ...\n","  [ 0.4923144  -0.64405024 -0.15923941 ...  0.03407398 -0.02638396\n","    1.6473213 ]\n","  [ 0.49269247 -0.63164103 -0.15247867 ...  0.02763009 -0.05691024\n","    1.6609404 ]\n","  [ 0.7587868   0.41652974 -1.6516763  ... -2.2075346   0.46042457\n","    0.6991449 ]]]\n","\n","Map Embedding Shape: (1, 256, 4096)\n","Map Embedding Array:\n"," [[[-0.01955852  0.9018345  -1.5668068  ... -0.85982764  0.4004894\n","    1.6175321 ]\n","  [-0.05273603  0.8952322  -1.3868546  ... -1.0558487   1.1358562\n","    1.7613556 ]\n","  [-0.13663417  0.6679472  -1.623211   ... -1.0674946   0.6367695\n","    1.7504694 ]\n","  ...\n","  [-0.26593617  0.46420035 -1.4571394  ... -1.073191    0.68581504\n","    1.777324  ]\n","  [ 0.01227309  0.88971245 -0.83627385 ... -1.2403903   1.3336465\n","    1.2411244 ]\n","  [-0.52278656  0.02359952 -0.29040745 ... -0.83638006  0.7411647\n","    0.70994335]]]\n"]}]},{"cell_type":"markdown","source":["# Bounding Box Results [orion.py](https://github.com/xiaomi-mlab/Orion/blob/2eddb627/mmcv/models/detectors/orion.py)\n","This file is saved as an array of dictionaries, where each dictionary contains the detection results for a single sample or time step. The common keys and their meanings are:\n","\n","* boxes_3d: 3D bounding boxes (LiDARInstance3DBoxes format)\n","* scores_3d: Confidence scores for each detection\n","* labels_3d: Class labels (0-8 for the 9 detection classes)\n","* trajs_3d (optional): Future trajectory predictions if motion forecasting is enabled\n","\n","# Lane Results [orion.py](https://github.com/xiaomi-mlab/Orion/blob/2eddb627/mmcv/models/detectors/orion.py)\n","* map_scores_3d: Confidence scores for detected lanes\n","* map_labels_3d: Lane class labels (6 classes: Broken, Solid, SolidSolid, Center, TrafficLight, StopSign)\n","* map_pts_3d: Lane control points (shape: [num_lanes, n_control, 3])"],"metadata":{"id":"DsdOXXOWS6zR"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"11f0a871","executionInfo":{"status":"ok","timestamp":1763021941055,"user_tz":480,"elapsed":266743,"user":{"displayName":"Rahhul Jayaprakash","userId":"02142374991309415946"}},"outputId":"5fd54e2d-2f1a-4007-f1d1-65606be3a452"},"source":["#printing boxes_3d, scores_3d, labels_3d, trajs_3d; map_scores_3d, map_labels_3d, map_pts_3d\n","%%bash\n","conda run -n orion_env python results-orion-outputs/scripts/visualize_detection_results.py"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Bounding Box Results Type: <class 'numpy.ndarray'>\n","Bounding Box Results Shape: (1,)\n","Bounding Box Results Array:\n"," [{'boxes_3d': LiDARInstance3DBoxes(\n","     tensor([[  4.5223, -20.0158,  -1.7567,  ...,   2.8503,  -2.7403,   8.9753],\n","         [  4.9073, -34.3515,  -1.4148,  ...,  -0.2639,   2.0223,  -7.8007],\n","         [ -7.2264,  23.0681,  -1.7680,  ...,   2.8710,  -0.9644,   3.6656],\n","         ...,\n","         [ -7.2264,  23.0681,  -1.7680,  ...,   2.8710,  -0.9644,   3.6656],\n","         [  1.1681,  -2.1684,  -1.7212,  ...,  -2.9380,  -0.1131,   0.1009],\n","         [  4.5223, -20.0158,  -1.7567,  ...,   2.8503,  -2.7403,   8.9753]])), 'scores_3d': tensor([0.9768, 0.9757, 0.9716, 0.9677, 0.1089, 0.1081, 0.1066, 0.1065, 0.1041,\n","         0.1039, 0.0933, 0.0848, 0.0833, 0.0813, 0.0793, 0.0770, 0.0750, 0.0737,\n","         0.0702, 0.0588, 0.0581, 0.0539, 0.0537, 0.0520, 0.0505, 0.0498, 0.0440,\n","         0.0331, 0.0324, 0.0311, 0.0307, 0.0281, 0.0276, 0.0261, 0.0249, 0.0226,\n","         0.0224, 0.0218, 0.0218, 0.0217, 0.0205, 0.0204, 0.0203, 0.0200, 0.0199,\n","         0.0199, 0.0198, 0.0184, 0.0158, 0.0148, 0.0144, 0.0131, 0.0124, 0.0114,\n","         0.0111, 0.0110, 0.0110, 0.0107, 0.0106, 0.0106, 0.0103, 0.0103, 0.0103,\n","         0.0101, 0.0099, 0.0096, 0.0096, 0.0095, 0.0095, 0.0093, 0.0090, 0.0089,\n","         0.0088, 0.0087, 0.0087, 0.0083, 0.0082, 0.0081, 0.0077, 0.0077, 0.0076,\n","         0.0076, 0.0075, 0.0072, 0.0072, 0.0071, 0.0071, 0.0069, 0.0067, 0.0066,\n","         0.0065, 0.0065, 0.0065, 0.0064, 0.0064, 0.0064, 0.0064, 0.0063, 0.0063,\n","         0.0063, 0.0062, 0.0062, 0.0062, 0.0061, 0.0061, 0.0060, 0.0060, 0.0060,\n","         0.0059, 0.0058, 0.0058, 0.0058, 0.0058, 0.0058, 0.0057, 0.0057, 0.0057,\n","         0.0057, 0.0057, 0.0056, 0.0056, 0.0056, 0.0055, 0.0055, 0.0055, 0.0055,\n","         0.0055, 0.0055, 0.0055, 0.0055, 0.0055, 0.0055, 0.0055, 0.0054, 0.0054,\n","         0.0054, 0.0054, 0.0054, 0.0054, 0.0053, 0.0053, 0.0053, 0.0053, 0.0053,\n","         0.0053, 0.0053, 0.0053, 0.0053, 0.0053, 0.0052, 0.0052, 0.0052, 0.0052,\n","         0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0052, 0.0051, 0.0051, 0.0051,\n","         0.0050, 0.0050, 0.0050, 0.0050, 0.0050, 0.0049, 0.0049, 0.0049, 0.0049,\n","         0.0048, 0.0048, 0.0048, 0.0048, 0.0048, 0.0047, 0.0047, 0.0047, 0.0047,\n","         0.0046, 0.0046, 0.0045, 0.0044, 0.0044, 0.0043, 0.0043, 0.0043, 0.0042,\n","         0.0042, 0.0041, 0.0041, 0.0041, 0.0040, 0.0040, 0.0040, 0.0040, 0.0040,\n","         0.0040, 0.0039, 0.0039, 0.0038, 0.0038, 0.0038, 0.0038, 0.0038, 0.0037,\n","         0.0037, 0.0037, 0.0037, 0.0037, 0.0037, 0.0036, 0.0036, 0.0035, 0.0035,\n","         0.0035, 0.0035, 0.0035, 0.0035, 0.0035, 0.0034, 0.0034, 0.0034, 0.0034,\n","         0.0034, 0.0033, 0.0033, 0.0032, 0.0032, 0.0032, 0.0032, 0.0031, 0.0031,\n","         0.0031, 0.0031, 0.0031, 0.0031, 0.0030, 0.0030, 0.0030, 0.0030, 0.0030,\n","         0.0030, 0.0029, 0.0029, 0.0029, 0.0029, 0.0029, 0.0029, 0.0029, 0.0029,\n","         0.0029, 0.0029, 0.0029, 0.0028, 0.0028, 0.0028, 0.0028, 0.0028, 0.0027,\n","         0.0027, 0.0027, 0.0027, 0.0027, 0.0027, 0.0027, 0.0027, 0.8417, 0.1793,\n","         0.0139, 0.0125, 0.0120, 0.0099, 0.0088, 0.0074, 0.0074, 0.0067, 0.0067,\n","         0.0061, 0.0057, 0.0053, 0.0051, 0.0050, 0.0049, 0.0047, 0.0044, 0.0044,\n","         0.0038, 0.0038, 0.0037, 0.0036, 0.0032, 0.0032, 0.0028, 0.0027, 0.0034,\n","         0.0031, 0.0033, 0.0040]), 'labels_3d': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n","         4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 7, 8]), 'trajs_3d': tensor([[[-1.4038e+00,  4.3609e+00, -1.1341e+00,  ...,  3.7804e+00,\n","           -7.0540e-01,  3.6582e+00],\n","          [-1.4555e+00,  4.6386e+00, -1.2362e+00,  ...,  3.7678e+00,\n","           -8.1677e-01,  3.6138e+00],\n","          [-1.3857e+00,  4.4185e+00, -8.7612e-01,  ...,  1.1163e+00,\n","           -3.9494e-01,  1.4340e+00],\n","          [-1.4289e+00,  4.6029e+00, -1.0443e+00,  ...,  2.5187e+00,\n","           -6.0005e-01,  2.5151e+00],\n","          [-1.4519e+00,  4.7998e+00, -1.3606e+00,  ...,  4.5806e+00,\n","           -1.0514e+00,  4.3618e+00],\n","          [-1.4480e+00,  4.7192e+00, -1.2856e+00,  ...,  4.4674e+00,\n","           -9.4741e-01,  4.3255e+00]],\n","\n","         [[ 1.0901e+00, -3.7717e+00,  1.1104e+00,  ..., -3.6289e+00,\n","            1.0683e+00, -3.3385e+00],\n","          [ 1.1891e+00, -3.6800e+00,  1.2629e+00,  ..., -3.6125e+00,\n","            1.3759e+00, -3.1799e+00],\n","          [ 1.0716e+00, -3.8277e+00,  1.1005e+00,  ..., -3.7910e+00,\n","            1.1501e+00, -3.6676e+00],\n","          [ 1.1242e+00, -3.7589e+00,  1.1808e+00,  ..., -3.7182e+00,\n","            1.2266e+00, -3.4412e+00],\n","          [ 8.3375e-01, -3.5290e+00,  8.9624e-01,  ..., -2.9270e+00,\n","            7.1876e-01, -1.7238e+00],\n","          [ 1.0560e+00, -3.6265e+00,  1.1223e+00,  ..., -3.3750e+00,\n","            1.0635e+00, -2.4166e+00]],\n","\n","         [[-6.5441e-01,  2.7018e+00, -8.4311e-01,  ...,  3.8257e+00,\n","           -7.8211e-01,  3.9559e+00],\n","          [-6.9784e-01,  2.6317e+00, -8.8154e-01,  ...,  3.6314e+00,\n","           -1.0123e+00,  3.8122e+00],\n","          [-4.2177e-01,  1.5327e+00, -5.3502e-01,  ...,  3.0774e+00,\n","           -6.9657e-01,  3.3589e+00],\n","          [-5.6912e-01,  2.2500e+00, -7.0841e-01,  ...,  3.4924e+00,\n","           -8.7855e-01,  3.7077e+00],\n","          [-8.5051e-01,  3.3629e+00, -1.1029e+00,  ...,  4.6695e+00,\n","           -1.1303e+00,  4.7903e+00],\n","          [-7.6533e-01,  3.0531e+00, -9.7802e-01,  ...,  4.0109e+00,\n","           -1.0232e+00,  4.1447e+00]],\n","\n","         ...,\n","\n","         [[-6.5441e-01,  2.7018e+00, -8.4311e-01,  ...,  3.8257e+00,\n","           -7.8211e-01,  3.9559e+00],\n","          [-6.9784e-01,  2.6317e+00, -8.8154e-01,  ...,  3.6314e+00,\n","           -1.0123e+00,  3.8122e+00],\n","          [-4.2177e-01,  1.5327e+00, -5.3502e-01,  ...,  3.0774e+00,\n","           -6.9657e-01,  3.3589e+00],\n","          [-5.6912e-01,  2.2500e+00, -7.0841e-01,  ...,  3.4924e+00,\n","           -8.7855e-01,  3.7077e+00],\n","          [-8.5051e-01,  3.3629e+00, -1.1029e+00,  ...,  4.6695e+00,\n","           -1.1303e+00,  4.7903e+00],\n","          [-7.6533e-01,  3.0531e+00, -9.7802e-01,  ...,  4.0109e+00,\n","           -1.0232e+00,  4.1447e+00]],\n","\n","         [[-1.0901e-01,  4.3301e-01, -8.9917e-02,  ...,  5.2258e-01,\n","           -1.2348e-01,  6.2511e-01],\n","          [-5.5150e-02,  1.4177e-01, -5.0626e-02,  ...,  3.1801e-01,\n","           -1.1225e-01,  4.3478e-01],\n","          [-1.2247e-03, -2.7415e-03,  2.1980e-05,  ..., -6.7929e-04,\n","            5.2286e-04, -3.4663e-03],\n","          [-4.9971e-03,  4.7005e-03, -3.5868e-03,  ...,  9.4185e-03,\n","           -5.2005e-03,  9.0012e-03],\n","          [-4.5054e-01,  1.1308e+00, -5.3928e-01,  ...,  2.1094e+00,\n","           -8.4143e-01,  2.1617e+00],\n","          [-1.8041e-01,  6.8088e-01, -1.8754e-01,  ...,  9.9586e-01,\n","           -3.3051e-01,  1.0917e+00]],\n","\n","         [[-1.4038e+00,  4.3609e+00, -1.1341e+00,  ...,  3.7804e+00,\n","           -7.0540e-01,  3.6582e+00],\n","          [-1.4555e+00,  4.6386e+00, -1.2362e+00,  ...,  3.7678e+00,\n","           -8.1677e-01,  3.6138e+00],\n","          [-1.3857e+00,  4.4185e+00, -8.7612e-01,  ...,  1.1163e+00,\n","           -3.9494e-01,  1.4340e+00],\n","          [-1.4289e+00,  4.6029e+00, -1.0443e+00,  ...,  2.5187e+00,\n","           -6.0005e-01,  2.5151e+00],\n","          [-1.4519e+00,  4.7998e+00, -1.3606e+00,  ...,  4.5806e+00,\n","           -1.0514e+00,  4.3618e+00],\n","          [-1.4480e+00,  4.7192e+00, -1.2856e+00,  ...,  4.4674e+00,\n","           -9.4741e-01,  4.3255e+00]]])}                                                                                                                                    ]\n","\n","Lane Results Type: <class 'numpy.ndarray'>\n","Lane Results Shape: (1,)\n","Lane Results Array:\n"," [{'map_scores_3d': tensor([0.9850, 0.9581, 0.9466, 0.8414, 0.7082, 0.4951, 0.2387, 0.1687, 0.1611,\n","         0.1568, 0.1478, 0.1435, 0.9879, 0.9394, 0.8637, 0.8355, 0.7661, 0.7554,\n","         0.7110, 0.6255, 0.4879, 0.4491, 0.3617, 0.3268, 0.2523, 0.1901, 0.1803,\n","         0.1529, 0.1492, 0.9866, 0.9859, 0.9686, 0.9460, 0.9059, 0.8387, 0.8294,\n","         0.7667, 0.6114, 0.3756, 0.3557, 0.2519, 0.2094, 0.1994, 0.1952, 0.1842,\n","         0.1803, 0.1775, 0.1530, 0.1299, 0.5673]), 'map_labels_3d': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","         3, 5]), 'map_pts_3d': tensor([[[-2.4707e+00, -5.0781e+01,  1.6489e-02],\n","          [-5.3213e+00, -4.0668e+01,  1.7524e-03],\n","          [-8.4051e+00, -3.0848e+01,  3.4585e-03],\n","          ...,\n","          [-2.5309e+01,  3.0020e+01,  1.5158e-02],\n","          [-2.7812e+01,  4.0313e+01, -1.6675e-03],\n","          [-3.0577e+01,  5.1095e+01,  2.1949e-02]],\n","\n","         [[-2.9961e+01,  5.1192e+01,  2.2487e-02],\n","          [-2.6930e+01,  4.0912e+01,  8.0285e-03],\n","          [-2.4046e+01,  3.1135e+01,  1.1548e-02],\n","          ...,\n","          [-9.3294e+00, -2.9858e+01,  5.9810e-03],\n","          [-6.3921e+00, -3.9649e+01,  7.9894e-03],\n","          [-4.3029e+00, -5.0833e+01,  1.4653e-02]],\n","\n","         [[-3.0872e+01,  5.1189e+01,  2.5744e-03],\n","          [-2.8203e+01,  4.0708e+01, -3.3998e-03],\n","          [-2.5124e+01,  3.0843e+01, -5.3487e-03],\n","          ...,\n","          [-9.3078e+00, -3.0182e+01, -6.6481e-03],\n","          [-6.4320e+00, -4.0394e+01, -5.0864e-03],\n","          [-3.9392e+00, -5.1099e+01,  7.4496e-03]],\n","\n","         ...,\n","\n","         [[ 7.2881e+00, -3.6770e+01,  1.8432e-02],\n","          [ 5.2251e+00, -2.7957e+01,  1.5467e-02],\n","          [ 3.2234e+00, -1.8649e+01,  1.5451e-02],\n","          ...,\n","          [-9.8440e+00,  3.3509e+01,  1.6223e-02],\n","          [-1.2080e+01,  4.2200e+01,  2.6738e-02],\n","          [-1.4601e+01,  5.1122e+01,  1.6520e-02]],\n","\n","         [[-7.9134e+00, -4.7301e+01,  1.5913e-02],\n","          [-7.5762e+00, -4.7741e+01, -1.0043e-02],\n","          [-7.7552e+00, -4.8253e+01,  1.1754e-03],\n","          ...,\n","          [-6.7597e+00, -5.0357e+01,  1.5675e-02],\n","          [-6.4889e+00, -5.0722e+01, -1.1574e-02],\n","          [-6.5427e+00, -5.1196e+01, -2.9912e-03]],\n","\n","         [[ 1.9853e+01, -4.7228e+01,  1.9059e-02],\n","          [ 2.0132e+01, -4.6858e+01,  1.0603e-02],\n","          [ 2.0356e+01, -4.6221e+01,  2.0195e-02],\n","          ...,\n","          [ 2.2109e+01, -4.7693e+01,  3.9196e-02],\n","          [ 2.1505e+01, -4.8134e+01,  4.5614e-02],\n","          [ 2.0347e+01, -4.8019e+01,  2.0477e-02]]])}                                                                                              ]\n","\n"]}]},{"cell_type":"markdown","source":["#Reasoning Space\n","## Vision Token Concatenation\n","* Function: In simple_test_pts() [orion.py](https://github.com/xiaomi-mlab/Orion/blob/main/mmcv/models/detectors/orion.py)\n","\n","**Input:**\n","\n","* vision_embeded_obj: (1, 257, 4096) from OrionHead\n","* vision_embeded_map: (1, 256, 4096) from OrionHeadM\n","\n","**Processing:**\n","\n","* vision_embeded = torch.cat([vision_embeded_obj, vision_embeded_map], dim=1) [orion.py](https://github.com/xiaomi-mlab/Orion/blob/main/mmcv/models/detectors/orion.py)\n","\n","**Output:**\n","\n","* vision_embeded: (1, 513, 4096) tensor (dtype: float32 or float16)\n","## Text Input Preparation\n","* Pipeline: LoadAnnoatationCriticalVQATest transform [orion_stage3_agent.py](https://github.com/Rahhul17-IITH/Orion-v1/blob/main/adzoo/orion/configs/orion_stage3_agent.py)\n","\n","**Input:**\n","\n","* Critical QA prompts from dataset (e.g., \"Describe the driving scenario and plan waypoints\")\n","\n","**Processing:**\n","\n","* Tokenize using LLaVA tokenizer\n","* Format as conversation with special tokens\n","\n","**Output:**\n","\n","* input_ids: List of token sequences, each (seq_len,) tensor [orion.py](https://github.com/xiaomi-mlab/Orion/blob/main/mmcv/models/detectors/orion.py)\n","\n","## Multi-Turn Conversation Loop\n","* Function: Loop in simple_test_pts() [orion.py](https://github.com/xiaomi-mlab/Orion/blob/main/mmcv/models/detectors/orion.py)\n","\n","For each conversation turn:\n","\n","**Input:**\n","\n","* input_ids[i]: (1, turn_len) - current turn tokens orion.py:769\n","* history_input_output_id: List of previous turns [orion.py](https://github.com/xiaomi-mlab/Orion/blob/main/mmcv/models/detectors/orion.py)\n","\n","**Processing:**\n","\n","* Check for special waypoint token (<waypoint_ego>) [orion.py](https://github.com/xiaomi-mlab/Orion/blob/main/mmcv/models/detectors/orion.py)\n","* Concatenate conversation history: context_input_ids = torch.cat(history_input_output_id, dim=-1) orion.py:782\n","\n","**Output:**\n","\n","* context_input_ids: (1, total_seq_len) - full conversation context\n","## LLM Inference for Planning Token\n","* Function: lm_head.inference_ego() [orion.py](https://github.com/xiaomi-mlab/Orion/blob/main/mmcv/models/detectors/orion.py)\n","\n","**Input:**\n","\n","* inputs: (1, total_seq_len) - conversation tokens [orion.py](https://github.com/xiaomi-mlab/Orion/blob/main/mmcv/models/detectors/orion.py)\n","* images: (1, 513, 4096) - vision embeddings\n","* return_ego_feature=True\n","\n","**Processing:** (inside LlavaLlamaForCausalLM.inference_ego()) llava_llama.py:243-311 :\n","\n","* Merge vision and text: prepare_inputs_labels_for_multimodal() creates inputs_embeds of shape (1, 513+total_seq_len, 4096) [llava_llama.py](https://github.com/xiaomi-mlab/Orion/blob/2eddb627/mmcv/utils/llava_llama.py)\n","* LLaMA forward pass: self.model(inputs_embeds=inputs_embeds) → hidden_states of shape (1, 513+total_seq_len, 4096)\n","* Extract planning token: Find <waypoint_ego> token position and extract its hidden state\n","\n","**Output:**\n","\n","* ego_feature: (1, 4096) tensor (dtype: float32) - planning token [orion.py](https://github.com/xiaomi-mlab/Orion/blob/main/mmcv/models/detectors/orion.py)\n","## LLM Inference for Text Generation (Non-Planning Turns)\n","* Function: lm_head.generate()\n","\n","**Input:**\n","\n","* inputs: (1, total_seq_len) - conversation tokens\n","* images: (1, 513, 4096) - vision embeddings\n","* Generation params: temperature=0.1, top_p=0.75, max_new_tokens=320\n","\n","**Processing:**\n","\n","* Similar vision-text merging\n","* Autoregressive generation with sampling\n","\n","**Output:**\n","\n","* output_ids: (1, generated_len) tensor - generated token IDs [orion.py](https://github.com/xiaomi-mlab/Orion/blob/main/mmcv/models/detectors/orion.py)"],"metadata":{"id":"yFl0j4EC45Cs"}},{"cell_type":"code","source":["#printing ego_feature\n","!python results-orion-outputs/scripts/visualize_ego_feature.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bLjBnS2CLXrY","executionInfo":{"status":"ok","timestamp":1763020040904,"user_tz":480,"elapsed":2015,"user":{"displayName":"Rahhul Jayaprakash","userId":"02142374991309415946"}},"outputId":"22167e51-8eba-400b-e0de-9cd389fe339f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Ego Feature Shape: (1, 4096)\n","Ego Feature Array:\n"," [[-0.17627868  0.07835264 -0.06944825 ... -0.07497703 -0.5635871\n","  -0.2444535 ]]\n"]}]},{"cell_type":"markdown","source":["# Action Space\n","## Planning Token Preparation\n","**Function:** In simple_test_pts() [orion.py](https://github.com/xiaomi-mlab/Orion/blob/main/mmcv/models/detectors/orion.py)\n","\n","**Input:**\n","\n","* ego_feature: (1, 4096) tensor\n","\n","**Processing:**\n","\n","* Convert to float32: ego_feature = ego_feature.to(torch.float32) [orion.py](https://github.com/xiaomi-mlab/Orion/blob/main/mmcv/models/detectors/orion.py)\n","* Add time dimension: current_states = ego_feature.unsqueeze(1) → (1, 1, 4096)\n","\n","**Output:**\n","\n","* current_states: (1, 1, 4096) tensor\n","\n","# Trajectory Generation (VAE Mode)\n","* Function: VAE-based planning in simple_test_pts() [orion.py](https://github.com/xiaomi-mlab/Orion/blob/main/mmcv/models/detectors/orion.py)\n","\n","**Input:**\n","\n","* current_states: (1, 1, 4096)\n","\n","**Processing:**\n","\n","* Distribution sampling: distribution_forward(current_states, None, None) → sample of shape (1, 32) (latent code) [orion.py](https://github.com/xiaomi-mlab/Orion/blob/main/mmcv/models/detectors/orion.py)\n","* Future state prediction: future_states_predict(B, sample, hidden_states, current_states) → states_hs of shape (6, 1, 1, 4096) (6 timesteps)\n","* Trajectory decoding: For each timestep, ego_fut_decoder(ego_query_hs[i]) → (1, 6, 2) (6 modes, 2D coords)\n","* Stack timesteps: ego_fut_preds = torch.stack(ego_fut_trajs_list, dim=2) → (1, 6, 6, 2)"],"metadata":{"id":"CqbOibF85Omh"}},{"cell_type":"code","source":["#printing ego_fut_preds\n","!python results-orion-outputs/scripts/visualize_trajectory.py"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763020505928,"user_tz":480,"elapsed":815,"user":{"displayName":"Rahhul Jayaprakash","userId":"02142374991309415946"}},"outputId":"934560ec-7158-4dcb-85f0-a3b805246eac","id":"eLGRloYQfwY5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Trajectory Shape: (1, 6, 6, 2)\n","Trajectory Array:\n"," [[[[-0.20913687  3.5807621 ]\n","   [-1.0376061   3.764699  ]\n","   [-1.3528324   3.6801393 ]\n","   [-1.2351054   3.6720114 ]\n","   [-1.2603774   3.6748312 ]\n","   [-1.2735626   3.6967483 ]]\n","\n","  [[ 1.057352    3.0138216 ]\n","   [ 1.7058829   2.3560398 ]\n","   [ 1.9005634   2.1520894 ]\n","   [ 1.8582957   2.1411152 ]\n","   [ 1.9006087   2.1504965 ]\n","   [ 1.9230629   2.2301652 ]]\n","\n","  [[-0.17620757  3.5128942 ]\n","   [-1.1287831   3.6970603 ]\n","   [-1.3341503   3.749539  ]\n","   [-1.2232906   3.70322   ]\n","   [-1.2255021   3.7313988 ]\n","   [-1.2195609   3.792561  ]]\n","\n","  [[-0.04183037  3.5330524 ]\n","   [-1.0285923   3.6404078 ]\n","   [-1.3653128   3.6833303 ]\n","   [-1.2526083   3.676955  ]\n","   [-1.2725302   3.6854796 ]\n","   [-1.2821392   3.7095413 ]]\n","\n","  [[-0.09738618  3.4222233 ]\n","   [-1.5524168   3.805614  ]\n","   [-1.8910606   4.282611  ]\n","   [-1.7400434   4.3998213 ]\n","   [-1.6791778   4.5095673 ]\n","   [-1.619999    4.524088  ]]\n","\n","  [[ 0.0781251   3.640977  ]\n","   [-0.45696583  3.7615323 ]\n","   [-1.1823512   3.6433525 ]\n","   [-1.254634    3.6694653 ]\n","   [-1.3181078   3.7315044 ]\n","   [-1.288977    3.7996554 ]]]]\n"]}]}]}